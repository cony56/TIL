## LIME(Local Interpretable Model-Agnostic Explanations)



### Lime이란?



'Local Interpretable' + 'Model-Agnostic'

국소적으로 데이터에 대한 해석이 가능하고, 어떠한 모델이든 변수에 대한 진단이 가능함

-> 개별 데이터의 예측에 영향을 준 변수를 확인할 수 있음



### 모델의 결과값

### ![Lime_1(0612)](image/Lime_1(0612).png)



-> 개별 데이터에 대해 분류모델이 Label 값을 예측하도록 기여한 변수들과 그 영향력을 파악할 수 있음



### 모델 연구배경



* “Why Should I Trust You?” - Explaining the Predictions of Any Classifier 라는 논문에서 처음 소개됨
* 모델에 대한 신뢰성이 중요한 의학 진단, 테러리즘 예측에선 블랙박스 모형을 쓸 수 없음
* 각각의 테스트 데이터에 대해서 어떤 변수들이 모델의 판단에 영향을 주었는지 찾는 모델 개발



### 모델 파라미터 및 설명



 ![Lime_2(0612)](image/Lime_2(0612).png)

 ![Lime_3(0612)](image/Lime_3(0612).png)

* Loss Function : f와 g 함수의 결과값 사이의 거리를 최소화 

* 목적 함수 : 거리가 최소화되는 지점의 g 함수를 구함

* f : 기존 블랙박스 모형에 대해  (입력값 array)를 input으로, (predict 값)을 output으로 하는 함수, output은 label에 대한 확률값

* x : 변수들의 설명력을 확인하고자 하는 데이터의 입력값 array

* z : x 인근에 있는 샘플링된 입력값 array

* g: x array를 input으로 하며 입력값 array와 weight의 선형결합으로 이루어진 함수 
  $$
  g(x) = w_{1}*x_{1} + w_{2}*x_{2} + ...
  $$
  
* Ω(g) : g 함수의 복잡도
* πx(z) : exponential kernel 함수 -> (x.z 사이의 거리함수)^2 / σ^2

-> 샘플링 시 인접 데이터에 가중치를 주기위해 사용



### 모델 작동 원리



1) 변수의 설명력을 확인하고 싶은 데이터를 기준으로 정규분포로 값을 샘플링함

 ![Lime_4(0612)](image/Lime_4(0612).png)

2) 이 때 Exponential kernel을 통해 z array와의 거리를 기준으로 가중치를 줌

3) f(z) - g(z')을 최소화하게 학습

f(z) -> 기존 데이터를 입력하여 예측된 값 

g(z') -> 샘플링된 데이터를 입력하여 나온 값

-> 두 함수의 결과값의 차이를 최소화하는 손실함수에 따라 g(z)가 블랙박스 모델 f(x)의 local 영역에 대해

모델의 결정 경계가 됨

![Lime_5(0612)](image/Lime_5(0612).png)

4) 이 때 결정된 g(x)의 weight 값이 각 변수의 설명력을 나타내게 됨

ex) w1(column_1) = 0.7, w2(column_2) = -0.3



### 이미지, 텍스트 데이터에서의 샘플링 방식의 차이

* Tabular 데이터와 달리 이미지, 텍스트 데이터의 input은 (0,1)로 이루어짐
* 이미지에 대해선 특정 픽셀을 모아서 n개의 조합으로 나눔
* 샘플링 시 Tabular 데이터와 달리 해당 조합 내의 픽셀을 마스킹 처리를 하여 g(x)의 입력값으로 씀
* 이후 동일한 방식으로 최상의 픽셀 조합을 찾게끔 학습함



### Lime 모델의 장점

* 선형 모델이기 때문에 다른 XAI 모델에 비해 가볍고 속도가 빠름
* 모델 진단을 위한 모형이기 때문에 딥러닝, 머신러닝과 관계없이 모든 블랙박스 알고리즘에 적용가능함
* 텍스트, 이미지 데이터에도 적용 가능



### Lime 모델의 단점

* neighborhood의 개념을 결정짓기 어려움

-> kernel_width를 몇으로 하는게 좋은지에 대해 정답 셋이 없음, 

Lime 모델의 결과값을 도메인 전문가가 확인하고 합당한지 확인하는 것이 kernel_width가 타당한지 확인할 수 있는 유일한 방법임

* 모델 f의 결정 경계를 확정짓는 방식이 비결정적임

-> 입력 데이터가 같아도 샘플링이 랜덤하게 이루어지기 때문에 출력결과가 일정하지 않음

-> 출력 결과의 변화는 모델의 형태가 복잡할수록 더 커질 수 있음

* Lime은 개별 데이터에 대한 설명이기 때문에 모델 전체에 대한 설명의 일관성이 보장되지 않음 (Local 모델이 갖는 한계점)

-> 이 부분을 개선하고자 논문의 저자는 Submodular Pick이란 샘플링 방식을 고안함



### 출처

1. https://lime-ml.readthedocs.io/en/latest/lime.html
2. https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html
3. https://dodonam.tistory.com/202
4. https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b
5. https://homes.cs.washington.edu/~marcotcr/blog/lime/
6. https://christophm.github.io/interpretable-ml-book/lime.html#disadvantages-11
7. Marco Tulio Ribeiro, “Why Should I Trust You?” Explaining the Predictions of Any Classifier, KDD 2016 San Francisco, 2016
8.  안재현, XAI 설명 가능한 인공지능, 인공지능을 해부하다