### Scrapy 사용하기



pip 내 리눅스 명령어

`Ctrl`+`Z` : 파이썬 들어갔다 나올때 

`pip list` : 깔린 라이브러리 목록 볼 수 있음

`scrapy shell` : 스크래피 환경에 들어감

`fetch()` : Spider Opened / Crawled(200) '200'은 request 받는걸 성공해다는 뜻

`view(resoponse)` : True가 반한되면서 해당 사이트의 html을 바탕으로 링크를 띄어줌

`print(response.text)` : html 내용을 확인할 수 있음



* 크롤링과  스크래핑의 차이점

|      | 크롤링                                           | 스크래핑                         |
| ---- | ------------------------------------------------ | -------------------------------- |
|      | 자동화 Bot -> 일정한 규칙으로 웹 페이지 브라우징 | 웹 사이트에서 원하는 정보를 추출 |



* 웹페이지의 형식

-웹주소에서 물음표는 무조건 없거나 1개가 와야 함

Clint (web browser) - > server(요청)

물음표 앞에 값이 주소(리소스의 위치): https://news.naver.com/main/main.nhn

물음표 뒤 -> 전달 파라미터: ?mode=LSD&mid=shm&sid1=101#&date=%2000:00:00&page=1

(key)=(value)

여러개 파라미터 전달 시 구분 기호: &

-----> 웹페이지를 만들 때 html을 여러 개 만드는게 아니라 같은 html에 대해 컨텐츠만 바꿔서 DB에서 가져오는 것

### Scrapy에서 같은 형식의 여러 컨텐츠 가져오는 법

1)네이버 뉴스기사의 제목이 다음과 같은 xpath로 규칙이 있음

-li에 대한 index가 바뀌는 형태임

//*[@id="main_content"]/div[2]/ul[3]/li[1]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[3]/li[2]/dl/dt[2]/a

//*[@id="main_content"]/div[2]/ul[3]/li[3]/dl/dt[2]/a

``` python
response.xpath('//[@id="main_content"]/div[2]/ul/li/dl/dt[2]/a/text()').extract()

```

-->xpath의 규칙이 같게끔 모든 뉴스기사의 제목을 extract 해옴 



2) 뉴스 신문사 이름은 css스타일에서 개발자가 writing이란 class를 사용함

\#css 스타일로 특정 클래스만 가져오기 -> 뉴스 신문사 이름만 따오기

```python
response.css('.writing::text').extract()
>>> ['이데일리', '채널A', '데일리안', '매일경제', '파이낸셜뉴스', '한국경제', '중앙일보', '부산일보', '서울경제', '연합뉴스', '뉴시스', '한국경제TV', '아이뉴스24', '조세일보', '머니투데이', '헤럴드경제', 'MBC', '데일리안', '파이낸셜뉴스', '파이낸셜뉴스', 'KBS', '아시아경제', '헤럴드경제', '중앙일보', '뉴시스', '이데일리', '머니투데이', '이데일리', '조선비즈', ' 서울신문', '한국경제TV', '연합뉴스', 'MBN', '뉴스1', 'TV조선', '조선일보', 'MBC', '헤럴드경제', '서울경제']
```



tutorial/

​	scrapy.cg # deploy coniguration file

​	tutorial/

​		__init__.py

​		



1) 프로젝트 시작

```bash
$ scrapy startproject (프로젝트이름)
```

2) 파일 visual studio code에 띄우기

```bash
$ code .
```

-> 해당 path에서 code . 찍고 Enter 하면 VS code에서 해당 디렉토리에 프로젝트 생성



3) 프로젝트에 스파이더 생성하기

```bash
$scrapy genspider (생성하고자 하는 봇이름) "(url링크)" 
```

4) items.py 에 가져올 데이터 등록

```python
#scrapy.Item으로부터 상속받음
class MyscraperItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    writer = scrapy.Field()
    preview = scrapy.Field()
```

5) mybots.py 수정

```python
#mybots.py에서 allowed doman = ['naver.com'] 으로 수정하기
class MybotsSpider(scrapy.Spider):
    name = 'mybots'
    allowed_domains = ['naver.com']
    start_urls = ['http://news.naver.com/main/clusterArticles.nhn?id=c_202101030810_00000030&mode=LSD&mid=shm&sid1=101/']

    def parse(self, response):
        titles = response.xpath('//[@id="main_content"]/div[2]/ul/li/dl/dt[2]/a/text()').extract()
        writers = response.css('.writing::text').extract()
        previews = response.css('.lede::text').extract()

        items = []
        # items에 대해 XPATH, CSS를 통해 추출한 데이터를 저장        
        for idx in range(len(titles)):
            item = MyscraperItem()
            item['title'] = titles[idx]
            item['writer'] = writers[idx]
            item['preview'] = previews[idx]

            items.append(item)

            return items
```

--> MyscraperItem() 이란 객체를 myscraper 파일로 부터 import하여 스크래핑한 값들을 저장하고

items라는 리스트에 저장한다.

6) settings.py 설정 수정 및 추가

```python
# Obey robots.txt rules
ROBOTSTXT_OBEY = False
```

-> True에서 False로 바꿈/ 개인 프로젝트이기 때문에

```python
FEED_FORMAT = "csv"
FEEED_URI = 'my_news.csv'
FEED_FORMAT = "json"
FEED_URI = 'my_news.json'
# csv파일 엑셀로 열 때만
#FEED_EXPORT_ENCODING = 'utf-8-sig'
FEED_EXPORT_ENCODING = 'utf-8'
```

-> 파일 하단에 추가하기

7) 파일 테스트 하기

```bash
$scrapy crawl mybots(봇이름)
```

-> 결과로 .json 혹은 .csv 파일이 저장됨



* https://app.diagrams.net/

-> 웹페이지 그림으로 표현할 때 사용하기



### paging 기능 활용방법

scrapy 도큐먼트 페이지에서  scrapy.Spider 검색하고 확인

* https://docs.scrapy.org/en/latest/



* mybots.py에 추가하기

```python
URL = 'http://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid1=101&sid2=258/&page=%s'
start_page = 1
start_urls = [URL % start_page]

def start_requests(self):
    for i in range(2):
        #request에 대한 생성자를 반환시킴
        # request를 실행한 다음에 parse함수를 실행시키게 연동시키는게 callback의 기능임
        yield Request(url=URL %(i + start_page),callback=self.parse)

```

-> 페이징을 위해 start_requests라는 함수를 만들어 단일 페이지가 아닌 여러 페이지를 스크래핑 함

* 함수를 보다가 Yield 와 callback 개념이 불명확해서 좀 더 찾아보기로 함

### 파이썬 Yield는 무엇인가?

출처: https://kkamikoon.tistory.com/90 [컴퓨터를 다루다] <- 참고한 페이지

#### generator(생성자)란

-Iterator를 생성해주는 함수이다

-> 일반함수와 다르지 않지만 yield를 씀

```python
def number_generator(n):
    print("Function Start")
    while n <6:
        yield n
        n+= 1
    print("Function End")
     
if __name__== "__main__":
    for i in number_generator(0):
        print(i)
>>> Function Start
>>>0
>>>1
>>>2
>>>3
>>>4
>>>5
>>>Function End
```

Return값을 보면 main-> number_getenrator(0)이 실행된 후

return도 안했는데 값이 계속 반환됨, 즉 yield n 할 때 n의 값이 출력됨

그 원리는 다음과 같음

![generator사진(1)](C:\Users\user\Desktop\자연어처리과정\TIL\work\generator사진(1).jpeg)



-> 여기서 보듯이 yield는 n의 값을 반환하고 함수로 다시 돌아가는 기능을 함

generator 함수가 yield를 만나면 generator 함수가 정지된 상태에서 반환 값을 next()를 호출한 쪽으로 전달함 -> 그대로 결과값이 return됨

이후 함수가 종료되지 않고 나머지 부분을 실행함/ 물론 이 때 함수 내에서 사용한 local 변수가 메모리에 그대로 유지됨



더 복잡하지만 이해가 잘되는 예시를 봐보자

```python
def generator_test(n):
    print("-=-=-=-=-=-= Generator Start =-=-=-=-=-=-")
     
    while(n <3):
        print("<< Before Yield >>")
        yield n
        n+= 1
        print("<< After Yield >>")
         
    print("-=-=-=-=-=-= Generator End =-=-=-=-=-=-")
     
if __name__== "__main__":
    print("---------- Main Function Start ----------")
     
    for i in generator_test(0):
        print("Start For))))))))))))")
        print("Yield i is : ", i)
        print("End For))))))))))))))")
         
    print("---------- Main Function End ----------")
#--------------------------------------------------------------------------
#결과값
---------- Main Function Start ----------
<< Before Yield >>
# yield를 만나 잠깐 generator함수가 멈추고 for문 내의 함수들을 실행해줌
Start For))))))))))))
Yield i is : 0
End For))))))))))))))
#값을 반환한 후 다시 generator 함수로 돌아가 나머지(n+=1, print After Yield)를 수행
<< After Yield >>
<< Before Yield >>
Start For))))))))))))
Yield i is : 1
End For))))))))))))))
<< After Yield >>
<< Before Yield >>
Start For))))))))))))
Yield i is : 2
End For))))))))))))))
<< After Yield >>
-=-=-=-=-=-= Generator End =-=-=-=-=-=-
---------- Main Function End ----------    
```

이전 예시와 같이 Yield를 만나면 값을 반환하고 Main 쪽의 코드를 실행한 후 다시 generator 함수로 돌아옴



* Callback은 내일 추가할 예정 