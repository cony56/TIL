## 모형 검증 및 최적화(Model Validation & Optimization)

#### 모델 검증

* 모델의 Variance, Bias 등을 통해 모델의 성능을 평가함



#### 모델 최적화

- 단일 파라미터 혹은 복수 파라미터의 조합으로 모델을 실험하며 최적의 예층 성능을 갖는 파라미터를 찾아냄



### Validation_curve

- 단일 하이퍼 파라미터를 최적화하는 방법론

1)  데이터 블러오기, 파라미터 범위 설정

```python
from sklearn.datasets import load_digits
from sklearn.svm import SVC
from sklearn.model_selection import validation_curve
# x,y 데이터 불러오기
digits = load_digits()
X, y = digits.data, digits.target
# log -6승~ -1승까지 범위를 10분할로 나누어 리스트화
param_range = np.logspace(-6,-1, 10)
```

2) validation_curve 함수로 하이퍼 파라미터가 변화할 때의 validation 값을 측정함

```python
%%time
train_scores, test_scores = \
    validation_curve(SVC(), X, y,
                     param_name="gamma", param_range=param_range,
                     cv=10, scoring="accuracy", n_jobs=1)
```

3) 시각화를 통해 Training Score와 Cross-Validation Score를 비교해본다.

```python
mpl.rcParams["font.family"] = 'DejaVu Sans'
plt.semilogx(param_range, train_scores_mean, label="Training score", color="r")
plt.fill_between(param_range, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2, color="r")
plt.semilogx(param_range, test_scores_mean,
             label="Cross-validation score", color="g")
plt.fill_between(param_range, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.2, color="g")
plt.legend(loc="best")
plt.title("Validation Curve with SVC")
plt.xlabel("$\gamma$")
plt.ylabel("Score")
plt.ylim(0.0, 1.1)
plt.show()
```



![모형최적화(20210520)](image/모형최적화(20210520).png)

4)  Validation Score의 해석



* validation curve는 파라미터를 통해 모델을 평가하는데 쓰일 뿐 튜닝을 하는데 쓰이면 안된다.

-> 만약 validation curve를 바탕으로 파라미터 튜닝을 한다면 모델의 일반화가 부족하고 편향된 값이 나와 test 했을 때 더 좋지 않다. 

-> 최적의 parameter를 구해서 모델에 쓰는게 목적이 아니면 왜 찾나?

ML model의 validation을 위해 쓴다. 

* training score와 cross-validation score 그래프가 비슷할수록 좋다.
* 만약 두 값이(t/s, c-v/s) 모두 낮으면 일반화가 많이 되었거나 모델이 단순해 underfitting되었다는 의미
* 만약 training score가 급격히 높아지지만 validation_curve가 계속 낮다면 모델이 복잡하거나 데이터가 적어 overfitting 되었다는 의미

5) Variance와 Bias의 Trade-off

![모형최적화_2(20210520)](image/모형최적화_2(20210520).png)

출처) https://modulabs-biomedical.github.io/Bias_vs_Variance

이 수식은 MSE(Mean Squared Error) 를 풀었을 때 추정값에 대한 Var+ Bias의 합이 된다는 것을 의미함

결국 분산과 편향은 서로 반비례하는 관계임을 알 수 있음

수식을 풀어보자면,

분산은 추정값과 추정값의 평균이 떨어진 정도의 제곱에 대한 평균

-> 즉 얼마나 우리가 예측한 값들이 분산되어 있는지를 보여줌

편향성은 추정값의 평균과 실제 모수와의 차이에 대한 제곱

-> 즉 예측값의 평균과 실제값의 거리를 보여주는 지표가 됨

이 두가지를 합하였을 때 추정치-모수의 제곱의 평균-> MSE가 되는 걸 알 수 있음



### Grid_Search

- 다양한 파라미터의 조합을 루프로 돌며 최적의 파라미터를 찾아내는 방법론

#### Grid_Search Input 코드

```python
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
params = {
    'n_neighbors' : [5, 25],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}
grid_kn = GridSearchCV(estimator = kn,
                        param_grid = params,
                        scoring = 'accuracy', 
                        cv = 5, 
                        verbose = 1,
                        n_jobs = -1)
grid_kn.fit(X_train, y_train)

```

* param_grid : dict 형태 -> value값을 리스트로 구성하여 테스트하고 싶은 파라미터 옵션들을 모두 넣어줌
* scoring : CV 시 parameter 튜닝의 지표로 쓸 값(ex. accuracy, roc_curve) 

https://scikit-learn.org/stable/modules/model_evaluation.html 에 들어가면 쓸 수 있는 모든 scoring metrics가 나옴

* cv : cross-validation의 폴드 개수
* verbose: 1로 하면 모델의 디테일 정보 표시해줌
* n_jobs :  -1  -> 가용한 모든 cpu를 사용해 다수의 모델을 동시에 돌림, n -> cpu 중 n개 사용



#### GridSearch Output 코드

```python
grid_kn.cv_results()
grid_kn.best_params()
grid_kn.best_score()
```

* cv_result: dictionary 내에 모델과 관련된 모든 값이 들어가 있음
* best_params: dictionary 형태로 딕셔너리를 최적 상태에서의 파라미터 조합을 반환함
* best_score: 최적 상태에서의 score값을 반환함



### Random Search

- 탐색 대상 구간 내의 후보 hyperparameter 값을 랜덤 샘플링을 통해 선정함

-> 성적이 안좋은 파라미터에 대해 불필요한 반복 수행 횟수를 줄일 수 있음

* Grid Search 보다 빠르고 불필요한 탐색을 덜함

### Bayesian Optimization

* 앞의 두 검색 방법과 달리 사전정보를 최적값 탐색에 반영하는게 핵심임

1) Surrogate Model

* 기존의 입력값으로 미지의 함수를 만든다

-> ((x1,f(x1), (x2),f(x2) ....) 

* x는 조합하고자 하는 parameter의 조합이고 f(x1)은 일반화 성능에 대한 점수임(아마도 accuracy, f1-score같은 지표인 거 같음)

-> Surrogate Model에는 두 개의 모수가 존재하는데 성능의 불확실성(표준편차), 일반화 성능(평균)임

2) Acquisition Function

다음에 들어갈 후보로 가장 유용한 하이퍼파라미터 집합을 구하는 함수

x* = argmax Acquisition function 

->이 function을 통해 유용하다고 판단되는 집합을 surrogate Model에 넣고 f(x)값인 일반화 성능값도 획득함

-> 이후 surrogate 모델을 다시 모델링함

* 이러한 작업을 계속하며 추정하는 Surrogate  모델이 실제 함수의 모형에 가까워지는게 목표임









참고문헌:

1) https://datascienceschool.net/03%20machine%20learning/14.01%20%EB%AA%A8%ED%98%95%20%EC%B5%9C%EC%A0%81%ED%99%94.html

2) https://www.geeksforgeeks.org/validation-curve/

3) https://modulabs-biomedical.github.io/Bias_vs_Variance

4) https://scikit-learn.org/stable/modules/model_evaluation.html

5) https://wooono.tistory.com/102